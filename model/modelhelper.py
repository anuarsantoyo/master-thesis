# -*- coding: utf-8 -*-
"""modelinitialisierung.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1VtelAhzX3F9E3IpkIZ-AcNjs4lbYyCbe
"""

import torch
from torch import distributions

# Transformation
def bij_transform(prime, lower, upper):
  """Recieves a prime value of type tensor in [-inf, inf] and returns value in [lower, upper]"""
  bij = 1 / (1 + torch.exp(-prime / upper))
  scale = upper - lower
  return scale * bij + lower

def bij_transform_inv(transf, lower, upper):
  """Inverse transformation - Recieves a value of type tensor in [lower, upper] and returns value in [-inf, inf]"""
  return -torch.log(((upper - lower) / (transf - lower) - 1) ** upper)


# Model Parameter
dict_model_param = {'lower': {'R0': 2, 'phi': 0, 'sigma': 0.00001, 'alpha': 0.001},
                    'upper': {'R0': 5, 'phi': 50, 'sigma': 0.5, 'alpha': 0.05},
                    'value': {'R0': 3.6, 'phi': 25, 'sigma': 0.1, 'alpha': 0.01},
                    'scale': {'R0': 0.8, 'phi': 10, 'sigma': 0.03, 'alpha': 0.01}}

def get_dict_model_param():
    return dict_model_param

def initialize_prime_param(param, device, dtype):
  value = dict_model_param['value'][param]
  lower = dict_model_param['lower'][param]
  upper = dict_model_param['upper'][param]
  prime = bij_transform_inv(torch.tensor(value, device=device, dtype=dtype), lower, upper).detach().clone().requires_grad_(True)
  return prime

def initialize_epsilon(num_observations, sigma, device, dtype):
  epsilon_t = torch.zeros(num_observations, device=device, dtype=dtype)
  epsilon_t[0] = torch.distributions.Normal(torch.tensor(0., requires_grad=False, device=device, dtype=dtype), sigma.detach()).rsample()
  for t in range(1, num_observations):
      epsilon_t[t] = torch.distributions.Normal(epsilon_t[t - 1].detach(), sigma.detach()).rsample()
  return epsilon_t.requires_grad_(True)

# Observations
def initialize_observations(df_observations, start='2020-02-26', end='2022-01-31', observations=['Number_of_deaths', 'Confirmed_cases', 'Admissions_hospital']):
  time_period = (df_observations['Date'] >= start) & (df_observations['Date'] < end)
  columns = ['Date'] + observations
  df_obs_filtered = df_observations.loc[time_period][columns].reset_index(drop=True)
  return df_obs_filtered

# Loss Functions

def calc_random_walk_loss(epsilon_t, sigma, device, dtype):
  """Takes epsilon_t and sigma as an input and returns the random walk loss."""
  days = len(epsilon_t)
  loc = epsilon_t[:days-1]
  scale = sigma * torch.ones(days - 1, device=device, dtype=dtype)
  mvn = distributions.multivariate_normal.MultivariateNormal(loc, scale_tril=torch.diag(scale))
  ll = mvn.log_prob(epsilon_t[1:days])
  return -ll

def calc_mse(expected, observed):
  diff = expected - observed
  square = diff.square()
  msr = square.mean()
  return msr

def calc_prior_loss(R0, sigma, phi, alpha, device, dtpye): # To Do - use for loop - flexible input parameter
  """Takes R0, sigma, phi and alpha as an input and calculates the prior loss.
  The prior loss is calculated by using the log-probability."""

  ll = torch.tensor(0.0, device=device, dtype=dtype)

  ll += distributions.normal.Normal(loc=torch.tensor(dict_model_param['value']['R0'], device=device, dtype=dtype), scale=torch.tensor(dict_model_param['scale']['R0'], device=device, dtype=dtype)).log_prob(R0)[0] # neg

  ll += distributions.normal.Normal(loc=torch.tensor(dict_model_param['value']['sigma'], device=device, dtype=dtype), scale=torch.tensor(dict_model_param['scale']['sigma'], device=device, dtype=dtype)).log_prob(sigma)[0]

  ll += distributions.normal.Normal(loc=torch.tensor(dict_model_param['value']['phi'], device=device, dtype=dtype), scale=torch.tensor(dict_model_param['scale']['phi'], device=device, dtype=dtype)).log_prob(phi)[0]

  ll += distributions.normal.Normal(loc=torch.tensor(dict_model_param['value']['alpha'], device=device, dtype=dtype), scale=torch.tensor(dict_model_param['scale']['alpha'], device=device, dtype=dtype)).log_prob(alpha)[0]

  return -ll
